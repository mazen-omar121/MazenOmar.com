
<html>
	<head>
		<title>Projects - by MAZEN OMAR</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<!-- Note: The "styleN" class below should match that of the banner element. -->
					<header id="header" class="alt style2">
						<a href="index.html" class="logo"><strong>Projects</strong> <span>by MAZEN OMAR</span></a>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
				<nav id="menu">
					<ul class="links">
						<li><a href="index.html">Home</a></li>
						<li><a href="elements.html">About me</a></li>
					</ul>
					<ul class="actions stacked">
						<li><a href="Mazen Omar CV.pdf" class="button primary fit">My Resume</a></li>
					</ul>
				</nav>
				<!-- Banner -->
				<!-- Note: The "styleN" class below should match that of the header element. -->
					<section id="banner" class="style2">
						<div class="inner">
							<span class="image">
								<img src="images/pic07.jpg" alt="" />
							</span>
							<header class="major">
								<h1>Final Year Project</h1>
							</header>
							<div class="content">
								<p>This final year project involved the development of a fully autonomous vehicle prototype on a 1/10th scale, designed to achieve Level 5 autonomy using artificial intelligence. The system integrates computer vision (OpenCV and YOLOv8), sensor fusion, and motor control to enable autonomous driving in a simulated environment. It features high-definition camera perception, obstacle detection, real-time mapping, and adaptive navigation, coordinated through Raspberry Pi and Arduino. Despite challenges in low-light performance and integration under complex conditions, the project demonstrates the potential of small-scale robotic vehicles in advancing AI-based urban transport solutions.</p>
							</div>
						</div>
					</section>

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<div class="inner">
									<header class="major">
										<h2>Project Description</h2>
									</header>
									<p>The project focuses on designing and developing an AI-powered autonomous vehicle capable of independent operation without human intervention. It combines visual perception, sensor integration, and control systems to simulate full autonomy (Level 5). The vehicleâ€™s core perception system utilizes OpenCV for image preprocessing and YOLOv8 for object and boundary detection, enabling environmental awareness and path recognition. Motor and steering systems are managed by an Arduino and Raspberry Pi in a dual-controller setup. A camera system captures high-resolution visual data, while ultrasonic sensors enhance spatial awareness through proximity sensing. The system was tested on a custom-built track replicating urban driving scenarios with curves and obstacles.</p>
								</div>
							</section>

						<!-- Two -->
							<section id="two" class="spotlights">
								<section>
									<a href="fi.pdf" class="image">
										<img src="images/fi2.png" alt="" data-position="center center" />
									</a>
									<div class="content">
										<div class="inner">
											<header class="major">
												<h3>Design Requirements</h3>
											</header>
											<p>
												Construct a 1/10th scale autonomous vehicle chassis.

Integrate computer vision using YOLOv8 and OpenCV for object and lane detection.

Employ Raspberry Pi and Arduino to control driving and perception systems.

Enable real-time proximity detection using ultrasonic sensors.

Design a closed simulation track to test operational accuracy and agility.

Maintain average system response time under 5 seconds.

Achieve target driving speed of up to 10 km/h with accurate path-following.</p>
											<ul class="actions">
												<li><a href="fi.pdf" class="button">Learn more</a></li>
											</ul>
										</div>
									</div>
								</section>
								<section>
									<a href="fi.pdf" class="image">
										<img src="images/fi3.png" alt="" data-position="top center" />
									</a>
									<div class="content">
										<div class="inner">
											<header class="major">
												<h3>Problem-Solving</h3>
											</header>
											<p>One of the major challenges encountered was ensuring precise object detection and reliable steering under variable lighting and environmental conditions. The integration between Raspberry Pi and Arduino introduced synchronization issues affecting real-time responsiveness. To address this, efficient image pre-processing pipelines were implemented to reduce frame delays. Training the YOLOv8 model on diverse datasets improved detection accuracy. Track design was iterated multiple times to simulate complex urban driving conditions and expose system weaknesses. Real-time decision-making was fine-tuned to balance safety with performance by calibrating sensor thresholds and optimizing control loop logic for fast response under dynamic inputs.</p>
											<ul class="actions">
												<li><a href="fi.pdf" class="button">Learn more</a></li>
											</ul>
										</div>
									</div>
								</section>
								<section>
									<a href="fi.pdf" class="image">
										<img src="images/fi.png" alt="" data-position="40% 25%" />
									</a>
									<div class="content">
										<div class="inner">
											<header class="major">
												<h3>Final Result</h3>
											</header>
											<p>The final system comprises:

A 1/10th scale robotic vehicle frame powered by a DC motor and servo for speed and steering.

High-definition camera with YOLOv8 object detection for identifying lanes, boundaries, and obstacles.

Raspberry Pi (for image processing and navigation control) and Arduino (for motor and sensor control) in a dual-control architecture.

Multiple ultrasonic sensors providing 360-degree proximity awareness.

A custom closed-loop track featuring curves, bottlenecks, and obstacles for performance testing.

A responsive, adaptive control system that achieved average response times under 5 seconds at operational speeds up to 10 km/h.

While lighting inconsistencies and sensor fusion limitations affected performance in edge cases, the project effectively demonstrated the integration of AI in autonomous mobility and provides a strong foundation for future smart transportation developments.</p>
											<ul class="actions">
												<li><a href="fi.pdf" class="button">Learn more</a></li>
											</ul>
										</div>
									</div>
								</section>
							</section>


				<!-- Footer -->
				<footer id="footer">
					<div class="inner">
						<ul class="icons">
							<li><a href="mailto:Mazen@graduate.utm.my" class="icon solid alt fa-envelope"><span class="label">Twitter</span></a></li>
						</ul>
						<ul class="copyright">
							<li>&copy; Mr.MAZEN</li><li>Done by: <a href="index.html">MAZEN OMAR</a></li>
						</ul>
					</div>
				</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>